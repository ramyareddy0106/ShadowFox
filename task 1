import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import reuters
from collections import defaultdict
import difflib

# Download NLTK dataset
nltk.download('reuters')
nltk.download('punkt')

# Load dataset (using Reuters corpus as an example)
corpus = " ".join(reuters.sents(fileid) for fileid in reuters.fileids())
tokenized_corpus = word_tokenize(corpus.lower())

# Create a vocabulary
tokenizer = Tokenizer()
tokenizer.fit_on_texts([corpus])
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# Generate sequences for training
sequence_length = 4
input_sequences = []
for i in range(len(tokenized_corpus) - sequence_length):
    seq = tokenized_corpus[i:i + sequence_length]
    input_sequences.append(seq)

# Convert words to numbers
input_sequences = [[word_index[word] for word in seq] for seq in input_sequences]
input_sequences = np.array(input_sequences)

X = input_sequences[:, :-1]
y = input_sequences[:, -1]

# Convert output labels to categorical
y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)

# Pad sequences
X = pad_sequences(X, maxlen=sequence_length - 1)
# Build the model
model = Sequential([
    Embedding(vocab_size, 100, input_length=sequence_length - 1),
    LSTM(128, return_sequences=True),
    LSTM(128),
    Dense(128, activation="relu"),
    Dense(vocab_size, activation="softmax")
])

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
model.fit(X, y, epochs=10, batch_size=64)
# Autocorrect using edit distance
def autocorrect(word, word_list):
    suggestions = difflib.get_close_matches(word, word_list, n=1)
    return suggestions[0] if suggestions else word

# Predict next word
def predict_next_word(input_text):
    words = word_tokenize(input_text.lower())
    words = [autocorrect(word, word_index.keys()) for word in words]
    input_seq = [word_index[word] for word in words if word in word_index]
    
    # Pad sequence
    input_seq = pad_sequences([input_seq], maxlen=sequence_length - 1)

    # Predict next word
    predicted_index = np.argmax(model.predict(input_seq), axis=-1)[0]
    predicted_word = tokenizer.index_word[predicted_index]

    return predicted_word

# Test the system
sentence = "the stock market is"
print("Predicted next word:", predict_next_word(sentence))
